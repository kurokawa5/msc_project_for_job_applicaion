# msc_project

# Title
Estimating user’s emotion from non-verbal behaviour in communication

# Abstract
Interactions between humans and computers will become more natural as computers perceive non- verbal communication, such as human emotions, and modify their conversational strategies. This paper focuses on developing a model for estimating users’ emotions in communication from multi- modal features to support application users with intelligent agents. For this purpose, we use a mul- timodal dataset containing features of the face, eyes, mouth and posture of participants observed in presentations and conversations in 14 videos. The dataset also contains English subtitles of what each participant said, and we perform sentiment analysis using a pre-trained BERT model to pre- pare user emotion categories. We extracted various features such as face, eyes, mouth and posture movements to estimate the emotional category of the user. From these features, we created clas- sification models for inferring Ekman-level and Group-level emotional categories using Pytorch’s three-layer neural network and random forest and evaluated the accuracy of the emotional category estimation. As a result of our experiments, multimodal models achieved a classification accuracy of 0.82 at the Ekman level and 0.96 at the Group level. Despite the limited amount of non-verbal information in the dataset used in this project, we believe that the model has an excellent potential to predict users’ emotions. In addition, we were able to collect non-verbal data from a video of a user talking to create an emotion estimation model. Using the same method for other videos, we obtained non-verbal information about the user’s face, hands, mouth, and posture, which we can use in future projects.
